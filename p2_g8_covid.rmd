---
title: "p2_g8_covid"
author: "g8"
date: "03/07/2020"
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
always_allow_html: true

editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Loading libraries  
```{r}

library(rtweet)
library(lubridate) #ymd_hms
#install.packages("magrittr") # package installations are only needed the first time you use it
#detach("package:dplyr")  
#install.packages("dplyr")    # alternative installation of the %>%
library(magrittr) # needs to be run every time you start R and want to use %>%
library(dplyr)    # alternatively, this also loads %>%  And glimpse



library(kableExtra)  # kable
library(knitr)
library(dplyr) # %>% and unnest_tokens
library(ggplot2)
 

#install.packages("stringr")   # Install stringr R package For Str_c
library("stringr")             # Load stringr R package
#install.packages("tibble")
library(tibble)
#install.packages("tidyverse")

library(tidyr)

library("tm")  # For DataframeSource
library(tidytext)  # For tidy

#install.packages("wordcloud")
library(wordcloud) # For wordcloud
#install.packages("wordcloud2") # For wordcloud2
library(gridExtra) # For grid.arrange

library(recommenderlab) 
library(dplyr)  # for spread


#install.packages("tidyverse") #Try replacing library(dplyr) with library(tidyverse). The spread function now lives in the tidyr package which is part of the tidyverse along with dplyr.
library(tidyverse) # for spread
library(textdata) # For afinn

library(bit64) # For Integer64
#install.packages("knitr")
#install.packages("rmarkdown")
library(rmarkdown)

#-----------------
#remove.packages('lubridate')
#install.packages('Rcpp', dependencies = TRUE)
#install.packages('lubridate', dependencies = TRUE)
library(lubridate)

library(caTools) # For sample.split
library(rpart)  # For rpart and tree plot
library(rpart.plot)
library(caret) # For trainControl

```

## Loading the data 


```{r Loading the data}

covid <- as_tibble(data.table::fread(str_c("d:/", "COVID.csv"), encoding= "UTF-8"))

str(covid)
glimpse(covid)

names(covid)
```

## preparation Date 

```{r}

names(covid)[names(covid) == "Screen Name"] <- "Screen_Name"
names(covid)[names(covid) == "Tweet Language"] <- "Tweet_Language"

# We need this name for  create VCorpus object
names(covid)[names(covid) == "Tweet Content"] <- "text"  
names(covid)[names(covid) == "Tweet Id"] <- "doc_id"  
covid$doc_id <- gsub('"', "", covid$doc_id, fixed = TRUE)
covid$doc_id <- as.integer64(as.character(covid$doc_id))



names(covid)[names(covid) == "Tweet Type"] <- "Tweet_Type"

names(covid)[names(covid) == "Tweet Posted Time (UTC)"] <- "time"

names(covid)[names(covid) == "Tweet Location"] <- "location"

covid$date_time= parse_date_time(covid$time,  '%d/%b/%Y %H:%M:%S')
covid$date <- as.Date(covid$date_time, format = "%Y.%m.%d")

```

## Exploratory Data Analysis (EDA)

``` {r  Exploratory Data Analysis (EDA) }

ggplot(covid,aes(x=date)) + geom_histogram(aes(y = (..count..)),binwidth=10) + 
labs(x="Date", y="number of tweets by Date ")


dt <- covid %>% group_by(location) %>% count()   %>% filter(n>=200) %>% arrange(desc(n)) %>% ungroup()
ggplot(dt, aes(x=reorder(location, n), y=n)) +
  geom_bar(stat="identity", fill="darkgreen") + coord_flip() +
  labs(x="", y="number of tweets by location ") +
  theme(legend.position = "none")
```


## Do covid tweet in Languages other than English?
Some tweets are in languages other than English. The “Undefined” ones are generally very short tweets

``` {r}

dt <- covid %>% group_by(Tweet_Language) %>% count()   # %>% rename("Tweet_Language"= "lang", n= "Number of Tweets")
names(dt)
kable(dt) %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "float_left")

ggplot(dt[1:10,], aes(x = Tweet_Language, y = n)) +
  geom_bar(stat = "identity") # visualization Language
```

## Remove all Twitter than English Language
```{r}

covid <- covid %>% filter(Tweet_Language=="English")

```



## Now, let’s see if which people were interesting enough for Covid

```{r}

p1 <- covid[c("Name")]

p1 <- p1 %>% filter( Name != "" ) %>% group_by (Name) %>% count()   %>% filter(n>=30) %>% arrange(desc(n)) %>% ungroup()
p1

ggplot(p1, aes(x=reorder(Name, n), y=n)) +
  geom_bar(stat="identity", fill="darkgreen") + coord_flip() +
  labs(x="", y="number of tweets retweeted ") +
  theme(legend.position = "none")

```

## Text mining

##1 - Prepare The tweet texts
``` {r}


kable(head(covid %>% select(Name, Tweet_Language, text), 20), format = "html") %>%
  kable_styling() %>%
  column_spec(1, bold = T, width = "2cm", border_right = T) %>%
  column_spec(2, bold = T, width = "2cm", border_right = T) %>%
  column_spec(3, width = "19cm")
# So text variable contains some Regex, such as \n for a new line and also \".
covid$text[c(1,1)]
#removing the \n as removePunctuation ,  URLs  ,the & sign + converting the tweet text into ascii to remove emoji’s


covid$text <- gsub('\"', " ", covid$text, fixed = TRUE)
covid$text <- gsub('#', " ", covid$text, fixed = TRUE)

covid$text <- str_replace_all(covid$text, "[\n]" , "") #remove new lines
covid$text <- str_replace_all(covid$text, "&amp", "") # rm ampersand

#URLs are always at the end and did not counts towards the 140 characters limit
covid$text <- str_replace_all(covid$text, "http.*" , "")

covid$text <- iconv(covid$text, "latin1", "ASCII", sub="")

covid$text[c(1,1)]

```

##2- Creating a VCorpus object
Corpora are collections of documents containing (natural language) text
 Corpus metadata contains corpus specific metadata in form of tag-value pairs

```{r}
#names(covid)
#names(all_Tweets)
#One column needs to have a unique document id (and must be named doc_id), one column must be named ‘text’
# we make that in advance

all_Tweets <- covid  #  %>% filter(Tweet_Type=="Tweet" )

all_Corpus <- DataframeSource(all_Tweets)
all_Corpus <- VCorpus(all_Corpus)
all_Corpus
content(all_Corpus[[3]])

```


##3 - 
# remove the English stopwords
#convert all characters into lower characters (no more capitals)
#remove numbers
#remove all English stopwords.
#remove punctuation
#strip whitespaces
# TermDocumentMatrix, which has all (remaining) terms as rows and all Documents (tweets) as columns
#This is a processing step which involves creating a data frame where each term(i.e. each word from our reviews) is a column, and each review is a row, with 

```{r}
print(sort(stopwords("en")))

CleanCorpus <- function(x){
  x <- tm_map(x, content_transformer(tolower))
  x <- tm_map(x, removeNumbers) #remove numbers before removing words. Otherwise "trump2016" leaves "trump"
  x <- tm_map(x, removeWords, tidytext::stop_words$word)
  x <- tm_map(x, removePunctuation)
  x <- tm_map(x, stripWhitespace)
  return(x)
}


# Extra words removed  
RemoveNames <- function(x) {
#   x <- str_replace_all(x, "coronavirus" , "")
  x <- tm_map(x, removeWords, c("coronavirus"))
  return(x)
}


# TermDocumentMatrix, which has all (remaining) terms as rows and all Documents (tweets) as columns

CreateTermsMatrix <- function(x) {
  x <- TermDocumentMatrix(x)
  x <- as.matrix(x)
  y <- rowSums(x)
  y <- sort(y, decreasing=TRUE)
  return(y)
}

all_Corpus <- CleanCorpus(all_Corpus)


All_tweeter_Freq <- CreateTermsMatrix(all_Corpus)



content(all_Corpus[[1]])
```


##4- The words that Twitter  used the most , make a Top20 of most used terms

```{r}
all_Word_used <- data.frame(word=names(All_tweeter_Freq), count=All_tweeter_Freq)



all_Word_used[1:30,] %>%
  ggplot(aes(x=(reorder(word, count)), y=count)) +
  geom_bar(stat='identity', fill="blue") + coord_flip() + theme(legend.position = "none") +
  labs(x="")


#As the names (especially ‘Coronavirus’ ) became very overwhelming in the wordcloud, 
# I removed the names (with the RemoveName function) from the corpus and created the Term Frequency Matrix again first.

all_Corpus <- RemoveNames(all_Corpus)
All_tweeter_Freq <- CreateTermsMatrix(all_Corpus)
all_Word_used <- data.frame(word=names(All_tweeter_Freq), count=All_tweeter_Freq)

all_Word_used[1:30,] %>%
  ggplot(aes(x=(reorder(word, count)), y=count)) +
  geom_bar(stat='identity', fill="blue") + coord_flip() + theme(legend.position = "none") +
  labs(x="")

```

##5 -   standard” wordcloud

``` {r}
set.seed(2018)
wordcloud(all_Word_used$word, all_Word_used$count, max.words = 100, scale=c(2.5,.5), random.color = TRUE, colors=brewer.pal(9,"Set1"))
#This wordcloud is interactive and I really started to actually like wordclouds

wordcloud2::wordcloud2(all_Word_used[1:100,], color = "random-light", backgroundColor = "grey", shuffle=FALSE, size=0.4)

```


``` { r #Comparison cloud}

```


##6 - Bigrams : we use the tidy() method to construct a table with one row per document, including the metadata (such as id and datetimestamp) as columns alongside the text. becuase a corpus is a flexible storage method for documents, but doesn’t lend itself to processing with tidy tools

``` {r}

# 6.1 Covid  bigrams Top 30

bigram_all_Tidy <- tidy(all_Corpus)

plotBigrams <- function(tibble, topN=30, title="", color="#FF1493"){
  x <- tibble %>% select(text) %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2)
  y <- x %>% count(bigram, sort = TRUE) %>% top_n(topN, wt=n) %>%
    ggplot(aes(x=reorder(bigram, n), y=n)) +
    geom_bar(stat='identity', fill=color) + coord_flip() +
    theme(legend.position="none") + labs(x="", title=title)
}

bi_all <- plotBigrams(bigram_all_Tidy, title="Covid  bigrams", color="blue")
bi_all

```



## Sentiment analysis
# --------------  1        The Bing lexicon (positive/negative, binary)
``` {r}
get_sentiments("bing")

# Positive and negative words used most frequently

DocMeta_all_twittes <- meta(all_Corpus)  

DocMeta_all_twittes$date <- date(DocMeta_all_twittes$date)

bigram_all_Tidy$date <- DocMeta_all_twittes$date


Words_covid <- bigram_all_Tidy %>% unnest_tokens(word, text)
Bing_covid <- Words_covid %>% inner_join(get_sentiments("bing"), by="word")


b1 <- Bing_covid   %>% count(word, sentiment, sort=TRUE) %>%
  group_by(sentiment) %>% arrange(desc(n)) %>% slice(1:20) %>%
  ggplot(aes(x=reorder(word, n), y=n)) +
  geom_col(aes(fill=sentiment), show.legend=FALSE) +
  coord_flip() +
  facet_wrap(~sentiment, scales="free_y") +
  labs(x="", y="number of times used", title="most used words") +
  scale_fill_manual(values = c("positive"="green", "negative"="red"))
b1


#names(Bing_covid)
```
# -------------- 2- Time series of sentiment 

``` {r}

  

time_all  <- Bing_covid %>% group_by(date) %>% count(sentiment) %>%
  spread(sentiment, n) %>% mutate(score=positive-negative) %>%
  ggplot(aes(x=date, y=score)) +
  scale_x_date(limits=c(as.Date("2019-12-01"), as.Date("2020-02-28")), date_breaks = "1 week", date_labels = "%b") +
  geom_line(stat="identity", col="blue") + geom_smooth(col="red")  + labs(title="Sentiment All Twitters")

#+ geom_smooth(col="red") 

Bing_covid1 <- Bing_covid %>% group_by(date) %>% count(sentiment)  %>%
  spread(sentiment, n) %>% mutate(score=positive-negative) 

time_all
```
# -------------- 3- The AFFIN lexicon (positive/negative, with scores)

```{r}
get_sentiments("afinn")

Afinn_covid <- Words_covid %>% inner_join(get_sentiments("afinn"), by="word")
t1 <- Afinn_covid %>% select(id, date, word, value) %>% filter(date=="2020-02-28")
 head(t1)

names(Afinn_covid)[names(Afinn_covid) == "value"] <- "score"
a11 <- Afinn_covid   %>% group_by(date) %>% summarise(score=sum(score))

a1 <- Afinn_covid  %>% group_by(date) %>% summarise(score=sum(score)) %>%
  ggplot(aes(x=date, y=score)) +
  scale_x_date(limits=c(as.Date("2019-12-01"), as.Date("2020-02-287")), date_breaks = "1 week", date_labels = "%b") +
  geom_line(stat="identity", col="blue") + geom_smooth(col="red") + labs(title="Sentiment All Twitter")

a1
```
# -------------- 4 - The nrc lexicon (2 sentiment categories, and 8 basic emotions)

```{r}
get_sentiments("nrc")

Nrc_covid <- Words_covid %>% inner_join(get_sentiments("nrc"), by="word")

n1 <- Nrc_covid  %>% count(sentiment) %>%
  ggplot(aes(x=sentiment, y=n, fill=sentiment)) +
  geom_bar(stat="identity") + coord_polar() +
  theme(legend.position = "left", axis.text.x = element_blank()) +
  geom_text(aes(label=n, y=n + 1000)) +
  labs(x="", y="", title="Covid 19 with nrc lexicon")
n1

# another  plot for The nrc lexicon
sentimentscores <- Nrc_covid  %>% count(sentiment)
ggplot(data=sentimentscores,aes(x=sentiment,y=n))+
  geom_bar(aes(fill=sentiment),stat = "identity")+
  theme(legend.position="none")+
  xlab("Sentiments")+ylab("Scores")+
  ggtitle("Total sentiment based on scores")+
  theme_minimal()


```

# Classification Tree
## Preperation the dataset
We are going to splitting the table into one-token-per-row by unnest_tokens
than join with The Bing lexicon sentament
than make grouping with ID and Spread a key-value pair by spread function
and callculating the score between positive and nigative 
and covert the score >0 positive sentiment.  to True and <0 to  negative sentiment. False
the NA is neutral, but for simplicity purposes, we will only attempt to predict the positive and negative sentiment, and we will revisit neutral later This is because our goal is to train a model to recognize positive or negative language.
we Have 16566 False and 4597 True 



```{r}
Words_for_reg <- covid %>% unnest_tokens(word, text)
#Bing_covid_for_reg  <- Words_covid %>% inner_join(get_sentiments("bing"), by="word")

#c
Bing_covid_for_reg  <- Words_for_reg %>% inner_join(get_sentiments("bing"), by="word")
covid_with_sent_location <- Bing_covid_for_reg

names(Bing_covid_for_reg)[names(Bing_covid_for_reg) == "id"] <- "doc_id"
Bing_covid_for_reg <- Bing_covid_for_reg %>% group_by(doc_id) %>% count(sentiment)

Bing_covid_for_reg <- Bing_covid_for_reg %>%   spread(sentiment, n)
Bing_covid_for_reg[is.na(Bing_covid_for_reg)] <- 0
Bing_covid_for_reg <- Bing_covid_for_reg  %>% mutate(score=positive-negative)
Bing_covid_for_reg <- Bing_covid_for_reg %>%  mutate(sent_fin  = ifelse(score > 0 , "TRUE",ifelse(score <0 , "FALSE" , "NA")))

all_twittes_with_sent <- merge(x = covid, y = Bing_covid_for_reg, by = "doc_id", all.x = TRUE)
sum(is.na(all_twittes_with_sent$sent_fin)) #  neutral
#all_twittes_with_sent <- all_twittes_with_sent %>% drop_na()
all_twittes_with_sent <- all_twittes_with_sent %>% filter(!sent_fin=="NA")
all_twittes_with_sent$sent_fin = as.factor(all_twittes_with_sent$sent_fin)
table(all_twittes_with_sent$sent_fin)
```

# 2--- Creating input variables 

Clean Corpus by transforming it into lowercase, removing the puntuation and the common English words. 
build our Document Term Matrix. This is a processing step which involves creating a data frame where each term(i.e. each word from our reviews) is a column, and each review is a row, with corresponding values for the number of times each term appears in each review. We then remove infrequent terms and focus on the ones that appear in multiple reviews

```{r}
corpus_for_reg = VCorpus(VectorSource(all_twittes_with_sent$text))
corpus_for_reg = CleanCorpus(corpus_for_reg)
frequencies = DocumentTermMatrix(corpus_for_reg)
sparse = removeSparseTerms(frequencies, 0.99)
all_twitter_Sparse = as.data.frame(as.matrix(sparse))
colnames(all_twitter_Sparse) = make.names(colnames(all_twitter_Sparse))
```

# now add our dependent variable back and let's take a look at the result

```{r}
all_twitter_Sparse$sent_fin = all_twittes_with_sent$sent_fin

names (all_twitter_Sparse)

```
# Sentiment Analysis
## will start wil splitting the data into a training and testing set.

```{r}


split = sample.split(all_twitter_Sparse$sent_fin, SplitRatio = 0.7)
all_twitter_Sparse$split = split
train = subset(all_twitter_Sparse, split==TRUE)
test = subset(all_twitter_Sparse, split==FALSE)
table(train$sent_fin)


10669 /nrow(train)

```
# We then calculate our baseline accuracy that the model will have to surpass 70%
## Therefore the baseline accuracy is around 78%. This means that more than 3 quarters of all twitters are nigative Hence the dataset is biased towards nigative sentiment and the machine learning algorithm will also be more likely to predict nigative sentiment  of tweets.

# Classification Tree
## build a CART model (this stands for Classification and Regression Trees, in this case it will be classification). 
## it's much more interpretable and can be visualized. In this way we will be able to have a look at which words were treated as predictors.

```{r}




cartModel = rpart(sent_fin ~ ., data=train, method="class")
prp(cartModel)
```
# if the twitter contains patient , it will be immediately labelled as negative. Words such as protects lead to positive twitter

# evaluate the performance of CART.
```{r}
predictCART = predict(cartModel, newdata=test, type="class")
table(test$sent_fin, predictCART)
(4535 + 482)/nrow(test)
```

# The CART model has an 7% improvement over the baseline model which is a highly nigative sign

##Model Enhancement - Cross Validation
```{r}



numFolds=trainControl(method = "cv", number = 10)
cpGrid = expand.grid(.cp=seq(0.001, 0.01, 0.001))
train(sent_fin ~ ., data = train, method = "rpart", trControl = numFolds, tuneGrid = cpGrid)
```
# cp     Accuracy   Kappa    
  0.001  0.9017154  0.6739517
  0.002  0.8980705  0.6586304
  0.003  0.8941561  0.6445860

# The cross validation gave me the optimal parameter cp = 0.001 so I will now re-build the tree with this parameter

```{r}
cartModelImproved = rpart(sent_fin ~ ., data=train, method="class", cp= 0.001)
prp(cartModelImproved)

```
# gives us a good overview of which words are used to make the split decisions and hence which words contribute most to the positive/negative sentiment.

# Let's now obtain predictions using the new tree
```{r}

predictCARTImproved = predict(cartModelImproved, newdata=test, type="class")
table(test$sent_fin, predictCARTImproved)
(4426+1132)/nrow(test)
```

#  85%



# 
```{r}


Words_for_reg <- covid %>% unnest_tokens(word, text)
covid_with_sent_location  <- Words_for_reg %>% inner_join(get_sentiments("bing"), by="word")

covid_with_sent_location <- covid_with_sent_location %>% group_by(location,sentiment ) %>% 
  count() %>%  arrange(desc(n)) 
covid_with_sent_location <- covid_with_sent_location %>% filter(!location=="")


a11 <- covid_with_sent_location %>%
  arrange_(~ desc(n)) %>%
  group_by_(~ sentiment) %>%
  slice(1:20)

ggplot(a11, aes(x=location, y=n, fill=sentiment)) + geom_bar(stat='identity') + theme(axis.text.x=element_text(angle=45,hjust=1)) 


#textplot_network(a11, min_freq = 0.5, edge_color = "orange", edge_alpha = 0.8, edge_size = 5)

```

